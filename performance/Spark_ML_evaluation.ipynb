{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0709f1-9faa-434b-b23f-3a30285d977d",
   "metadata": {},
   "source": [
    "## Exploration of PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ddd12b7-af81-44f4-8166-fd8246a24406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.5.1\n",
      "  Using cached pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark==3.5.1)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4544a7a0-ac33-4979-9eb3-9d701f5110e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in ./anaconda3/lib/python3.11/site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f2565c-87c8-4d60-9b31-721fc396ae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/02 15:10:23 WARN Utils: Your hostname, MacBook-Air-91.local resolves to a loopback address: 127.0.0.1; using 172.20.10.5 instead (on interface en0)\n",
      "25/03/02 15:10:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/02 15:10:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651ccb87-19b4-4840-8d32-2beba0eeb015",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"sue\", 32),\n",
    "        (\"li\", 3),\n",
    "        (\"bob\", 75),\n",
    "        (\"heo\", 13),\n",
    "    ],\n",
    "    [\"first_name\", \"age\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b88854-ffe0-465b-b823-fbcb969ce201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|first_name|age|\n",
      "+----------+---+\n",
      "|       sue| 32|\n",
      "|        li|  3|\n",
      "|       bob| 75|\n",
      "|       heo| 13|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab1bfff5-1aaa-41c0-a191-fb1f69ac3fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9da761cc-9e8c-4a26-8ed3-fbbc217152b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyspark==2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99845dc2-81a3-49d6-8467-e0a8bff9ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a5a1455-d35e-4658-9e49-c397a3ecaca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, 2.0, 3.0, 10.0),\n",
    "    (2, 3.0, 5.0, 15.0),\n",
    "    (3, 4.0, 7.0, 20.0),\n",
    "    (4, 5.0, 9.0, 25.0),\n",
    "]\n",
    "columns = [\"id\", \"feature1\", \"feature2\", \"label\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f61a56d9-2142-4c9e-b73a-a383bf600f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
    "df = assembler.transform(df).select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19277efc-edb1-4a27-b32f-e22ef26b0334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/25 17:16:36 WARN Instrumentation: [fd061e9e] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/02/25 17:16:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/02/25 17:16:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/02/25 17:16:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6125c723-769f-4d3b-9c7e-2e1b05e1a496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [5.0,0.0]\n",
      "Intercept: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Coefficients: {model.coefficients}\")\n",
    "print(f\"Intercept: {model.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57704f65-64d0-436e-8037-b33d351bbc62",
   "metadata": {},
   "source": [
    "## Test Spark 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbdfc165-a77e-427e-bc09-6f3b15c7abe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.0.2 in /Users/nishkagovil/anaconda3/envs/spark3/lib/python3.8/site-packages (3.0.2)\n",
      "Requirement already satisfied: py4j==0.10.9 in /Users/nishkagovil/anaconda3/envs/spark3/lib/python3.8/site-packages (from pyspark==3.0.2) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5170afb-ac69-4fa7-9f45-109b5e26fcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sparkmeasure in /Users/nishkagovil/anaconda3/envs/spark3/lib/python3.8/site-packages (0.23.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sparkmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3f57ea-48e4-4371-80fb-375cf5cd4ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /Users/nishkagovil/anaconda3/envs/spark3/lib/python3.8/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8de9c150-c007-4e61-80a5-816d55933423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import time\n",
    "from sparkmeasure import StageMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e215187b-9632-4aa1-b941-d2e37f829366",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuration\n",
    "\n",
    "spark_version = \"3.0.2\"  # Change this for each test\n",
    "data_path = \"sample_libsvm_data.txt\"  # From spark examples\n",
    "num_iterations = 100\n",
    "\n",
    "# Initialize Spark with metrics collection\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(f\"LR-Perf-Test-{spark_version}\") \\\n",
    "    .config(\"spark.jars.packages\", \"ch.cern.sparkmeasure:spark-measure_2.12:0.23\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \n",
    "           \"-Ddev.ludovic.netlib.blas.nativeLib=libopenblas.dylib \" +\n",
    "           \"-Ddev.ludovic.netlib.blas.nativeLibPath=/opt/homebrew/Cellar/openblas/0.3.27_1/lib/\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \n",
    "           \"-Ddev.ludovic.netlib.blas.nativeLib=libopenblas.dylib\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "stagemetrics = StageMetrics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78206d83-2990-4a89-a962-49c0b419c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  102k  100  102k    0     0   357k      0 --:--:-- --:--:-- --:--:--  358k\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "curl -o sample_libsvm_data.txt https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\n",
    "mkdir -p data/mllib\n",
    "cp sample_libsvm_data.txt data/mllib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c39af4e-439f-46ca-b137-24dbf7ee6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Preparation\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "def load_data(spark_version, spark_session):\n",
    "    # Use absolute path for reliability\n",
    "    data_path = \"/Users/nishkagovil/Downloads/data/mllib/sample_libsvm_data.txt\"  # Update with your actual path\n",
    "    \n",
    "    if spark_version.startswith(\"1.\"):\n",
    "        return MLUtils.loadLibSVMFile(spark_session.sparkContext, data_path)\n",
    "    else:\n",
    "        return (spark_session.read\n",
    "                .format(\"libsvm\")\n",
    "                .option(\"numFeatures\", \"692\")  # This dataset has 692 features\n",
    "                .load(data_path))\n",
    "\n",
    "dataset = load_data(spark_version, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2182e81d-ee7f-40a0-a2f2-8c17f30983dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07cde25b-e837-4bc3-853d-60de6f280580",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training & Metrics Collection\n",
    "\n",
    "def train_model(data):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if spark_version.startswith(\"1.\"):\n",
    "        # Spark 1.0 (MLlib RDD API)\n",
    "        model = LogisticRegressionWithSGD.train(\n",
    "            data,\n",
    "            iterations=num_iterations,\n",
    "            regParam=0.01\n",
    "        )\n",
    "    else:\n",
    "        # Spark 2.0+ (ML DataFrame API)\n",
    "        lr = LogisticRegression(\n",
    "            maxIter=num_iterations,\n",
    "            regParam=0.01\n",
    "        )\n",
    "        model = lr.fit(data)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    return model, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c4015eb-0ccb-4637-ab49-678fb21ede21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute with metrics collection\n",
    "stagemetrics.begin()\n",
    "model, exec_time = train_model(dataset)\n",
    "metrics_df = stagemetrics.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7660563-5809-4bdf-85ff-6363ac5577a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Spark 3.0.2 Performance Metrics\n",
      "========================================\n",
      "\n",
      "Training Time: 0.72s\n",
      "\n",
      "Resource Utilization:\n",
      "• Stage Duration: 0.03s\n",
      "• CPU Time: 0.02s\n",
      "• Peak Memory: 0.00 MB\n",
      "• Executor Runtime: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/02 16:38:30 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "### Results Analysis\n",
    "\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"Spark {spark_version} Performance Metrics\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "print(f\"\\nTraining Time: {exec_time:.2f}s\")\n",
    "\n",
    "if stagemetrics:\n",
    "    try:\n",
    "        # Create DF using current schema\n",
    "        metrics_df = stagemetrics.create_stagemetrics_DF(\"PerfStageMetrics\")\n",
    "        \n",
    "        if metrics_df and metrics_df.count() > 0:\n",
    "            # Use actual existing columns from error message\n",
    "            resource_metrics = metrics_df.select(\n",
    "                \"stageDuration\", \n",
    "                \"executorCpuTime\",\n",
    "                \"executorRunTime\", \n",
    "                \"peakExecutionMemory\"\n",
    "            ).first()\n",
    "\n",
    "            print(f\"\\nResource Utilization:\")\n",
    "            print(f\"• Stage Duration: {resource_metrics['stageDuration']/1000:.2f}s\")\n",
    "            print(f\"• CPU Time: {resource_metrics['executorCpuTime']/1000:.2f}s\")\n",
    "            print(f\"• Peak Memory: {resource_metrics['peakExecutionMemory']/1024**2:.2f} MB\")\n",
    "            print(f\"• Executor Runtime: {resource_metrics['executorRunTime']/1000:.2f}s\")\n",
    "        else:\n",
    "            print(\"\\nNo metrics collected - verify Spark jobs executed\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nMetrics error: {str(e)}\")\n",
    "else:\n",
    "    print(\"\\nMetrics collector not initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55cfa861-9937-4655-a598-8b7127ce71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cd10a-4e02-4868-86cb-3729e85c7f7c",
   "metadata": {},
   "source": [
    "## Test Spark 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e48b98e-9bbd-4adc-ac38-a35f5064e10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==2.4.8 in /Users/nishkagovil/miniforge3/envs/spark2_x86/lib/python3.6/site-packages (2.4.8)\n",
      "Requirement already satisfied: py4j==0.10.7 in /Users/nishkagovil/miniforge3/envs/spark2_x86/lib/python3.6/site-packages (from pyspark==2.4.8) (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==2.4.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0332730-9161-40dc-ae73-c2f39acfffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sparkmeasure in /Users/nishkagovil/miniforge3/envs/spark2_x86/lib/python3.6/site-packages (0.23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sparkmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e586652e-6aad-4fda-89b2-10c9675c8beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /Users/nishkagovil/miniforge3/envs/spark2_x86/lib/python3.6/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d9918f-00c6-42b3-84df-be1d96ce87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import time\n",
    "from sparkmeasure import StageMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00444ed5-c657-41cc-b46f-79898de0f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuration\n",
    "\n",
    "spark_version = \"2.4.8\"  # Change this for each test\n",
    "data_path = \"sample_libsvm_data.txt\"  # From spark examples\n",
    "num_iterations = 100\n",
    "\n",
    "# Initialize Spark with metrics collection\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LR-Perf-Test-2.4.8\") \\\n",
    "    .config(\"spark.jars.packages\", \"ch.cern.sparkmeasure:spark-measure_2.11:0.16\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "stagemetrics = StageMetrics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e82cd6-da2a-49cc-a8b2-a01b11190590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  102k  100  102k    0     0   739k      0 --:--:-- --:--:-- --:--:--  746k\n"
     ]
    }
   ],
   "source": [
    "# %%sh\n",
    "# curl -o sample_libsvm_data.txt https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\n",
    "# mkdir -p data/mllib\n",
    "# cp sample_libsvm_data.txt data/mllib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea2b84d-83bf-4f00-9835-6c2152a53882",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Preparation\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "def load_data(spark_version, spark_session):\n",
    "    # Use file:/// prefix for local paths\n",
    "    data_path = \"file:///Users/nishkagovil/Downloads/data/mllib/sample_libsvm_data.txt\"\n",
    "    \n",
    "    if spark_version.startswith(\"1.\"):\n",
    "        return MLUtils.loadLibSVMFile(spark_session.sparkContext, data_path)\n",
    "    else:\n",
    "        return (spark_session.read\n",
    "                .format(\"libsvm\")\n",
    "                .option(\"numFeatures\", \"692\")\n",
    "                .load(data_path))\n",
    "dataset = load_data(spark_version, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dfb89a9-fe9f-4889-824f-29e2d71ca5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46bfe201-08c2-41ab-b5e5-309b8b0a3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training & Metrics Collection\n",
    "\n",
    "def train_model(data):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if spark_version.startswith(\"1.\"):\n",
    "        # Spark 1.0 (MLlib RDD API)\n",
    "        model = LogisticRegressionWithSGD.train(\n",
    "            data,\n",
    "            iterations=num_iterations,\n",
    "            regParam=0.01\n",
    "        )\n",
    "    else:\n",
    "        # Spark 2.0+ (ML DataFrame API)\n",
    "        lr = LogisticRegression(\n",
    "            maxIter=num_iterations,\n",
    "            regParam=0.01\n",
    "        )\n",
    "        model = lr.fit(data)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    return model, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9090bb9e-424d-41fb-8638-b7de4a2afab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute with metrics collection\n",
    "stagemetrics.begin()\n",
    "model, exec_time = train_model(dataset)\n",
    "metrics_df = stagemetrics.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5e80284-3d43-44bc-8ee5-16d67270afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Spark 2.4.8 Performance Metrics\n",
      "========================================\n",
      "\n",
      "Training Time: 4.27s\n",
      "\n",
      "Resource Utilization:\n",
      "• Stage Duration: 0.19s\n",
      "• CPU Time: 0.13s\n",
      "• Peak Memory: 0.00 MB\n",
      "• Executor Runtime: 0.15s\n"
     ]
    }
   ],
   "source": [
    "### Results Analysis\n",
    "\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"Spark {spark_version} Performance Metrics\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "print(f\"\\nTraining Time: {exec_time:.2f}s\")\n",
    "\n",
    "if stagemetrics:\n",
    "    try:\n",
    "        # Create DF using current schema\n",
    "        metrics_df = stagemetrics.create_stagemetrics_DF(\"PerfStageMetrics\")\n",
    "        \n",
    "        if metrics_df and metrics_df.count() > 0:\n",
    "            # Use actual existing columns from error message\n",
    "            resource_metrics = metrics_df.select(\n",
    "                \"stageDuration\", \n",
    "                \"executorCpuTime\",\n",
    "                \"executorRunTime\", \n",
    "                \"peakExecutionMemory\"\n",
    "            ).first()\n",
    "\n",
    "            print(f\"\\nResource Utilization:\")\n",
    "            print(f\"• Stage Duration: {resource_metrics['stageDuration']/1000:.2f}s\")\n",
    "            print(f\"• CPU Time: {resource_metrics['executorCpuTime']/1000:.2f}s\")\n",
    "            print(f\"• Peak Memory: {resource_metrics['peakExecutionMemory']/1024**2:.2f} MB\")\n",
    "            print(f\"• Executor Runtime: {resource_metrics['executorRunTime']/1000:.2f}s\")\n",
    "        else:\n",
    "            print(\"\\nNo metrics collected - verify Spark jobs executed\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nMetrics error: {str(e)}\")\n",
    "else:\n",
    "    print(\"\\nMetrics collector not initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a484aac3-1b5c-42b5-b8f5-9e80a7ae7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e98e9-3046-4c8b-8596-9c682792e013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
