{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e1bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 2.4.8\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"API_Enhancement_Tests_Spark2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d20f12",
   "metadata": {},
   "source": [
    "## API Benchmark Test Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b01f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation: Basic filtering, Execution time: 10.8832 seconds\n",
      "Operation: Grouping and aggregation, Execution time: 9.7304 seconds\n",
      "Operation: Window functions, Execution time: 16.1734 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Operation</th>\n",
       "      <th>Execution Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic filtering</td>\n",
       "      <td>10.883168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grouping and aggregation</td>\n",
       "      <td>9.730412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Window functions</td>\n",
       "      <td>16.173417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Operation  Execution Time (s)\n",
       "0           Basic filtering           10.883168\n",
       "1  Grouping and aggregation            9.730412\n",
       "2          Window functions           16.173417"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. DataFrame Operations Tests\n",
    "# For both Spark 2.x and 3.x notebooks\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Performance measurement function\n",
    "def measure_execution(operation_name, func):\n",
    "    start_time = time.time()\n",
    "    result = func()\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Operation: {operation_name}, Execution time: {execution_time:.4f} seconds\")\n",
    "    return result, execution_time\n",
    "\n",
    "# Create sample data\n",
    "def create_test_data(spark):\n",
    "    # Create a larger dataset for meaningful measurements\n",
    "    data = [(i, f\"name_{i}\", i % 100, i % 5) for i in range(10**6)]\n",
    "    df = spark.createDataFrame(data, [\"id\", \"name\", \"value\", \"category\"])\n",
    "    return df\n",
    "\n",
    "# Test operations\n",
    "test_df = create_test_data(spark)\n",
    "results = []\n",
    "\n",
    "# Test 1: Basic filtering\n",
    "filter_op, filter_time = measure_execution(\n",
    "    \"Basic filtering\",\n",
    "    lambda: test_df.filter(F.col(\"value\") > 50).count()\n",
    ")\n",
    "results.append((\"Basic filtering\", filter_time))\n",
    "\n",
    "# Test 2: Grouping and aggregation\n",
    "group_op, group_time = measure_execution(\n",
    "    \"Grouping and aggregation\",\n",
    "    lambda: test_df.groupBy(\"category\").agg(F.avg(\"value\").alias(\"avg_value\")).collect()\n",
    ")\n",
    "results.append((\"Grouping and aggregation\", group_time))\n",
    "\n",
    "# Test 3: Window functions\n",
    "window_op, window_time = measure_execution(\n",
    "    \"Window functions\",\n",
    "    lambda: test_df.withColumn(\n",
    "        \"rank\",\n",
    "        F.rank().over(Window.partitionBy(\"category\").orderBy(F.desc(\"value\")))\n",
    "    ).collect()\n",
    ")\n",
    "results.append((\"Window functions\", window_time))\n",
    "\n",
    "# Store results\n",
    "performance_results = pd.DataFrame(results, columns=[\"Operation\", \"Execution Time (s)\"])\n",
    "performance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c627ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Type Hints and API Ergonomics Tests\n",
    "# For Spark 2.x\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define schema explicitly in Spark 2.x\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df_manual_schema = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99fba84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Operation</th>\n",
       "      <th>Code Lines Spark 2.x</th>\n",
       "      <th>Code Lines Spark 3.x</th>\n",
       "      <th>Reduction (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Type Hints &amp; Schema Definition</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Operation  Code Lines Spark 2.x  Code Lines Spark 3.x  \\\n",
       "0  Type Hints & Schema Definition                     4                     3   \n",
       "\n",
       "   Reduction (%)  \n",
       "0           25.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. API Usage Metrics\n",
    "# Track API usage metrics\n",
    "api_metrics = []\n",
    "\n",
    "import ast\n",
    "\n",
    "# Spark 2.x Code (More Verbose)\n",
    "spark2_code = \"\"\"\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df_manual_schema = spark.createDataFrame(data, schema)\n",
    "\"\"\"\n",
    "\n",
    "# Spark 3.x Code (With Type Hints, Optimized for Fewer Lines)\n",
    "spark3_code = \"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df_typed = spark.createDataFrame(data, schema=[\"name: string\", \"age: int\"])\n",
    "\"\"\"\n",
    "\n",
    "def count_code_lines(code_string):\n",
    "    \"\"\"Counts the number of executable statements in a code string.\"\"\"\n",
    "    tree = ast.parse(code_string)  # Parse code\n",
    "    return sum(isinstance(node, ast.stmt) for node in ast.walk(tree))\n",
    "\n",
    "# Count lines again\n",
    "code_lines_spark2 = count_code_lines(spark2_code)\n",
    "code_lines_spark3 = count_code_lines(spark3_code)\n",
    "\n",
    "# Store API Metrics\n",
    "api_metrics = [{\n",
    "    \"Operation\": \"Type Hints & Schema Definition\",\n",
    "    \"Code Lines Spark 2.x\": code_lines_spark2,\n",
    "    \"Code Lines Spark 3.x\": code_lines_spark3,\n",
    "    \"Reduction (%)\": round((1 - code_lines_spark3/code_lines_spark2) * 100, 2)\n",
    "}]\n",
    "\n",
    "# Create DataFrame\n",
    "df_api_metrics = pd.DataFrame(api_metrics)\n",
    "\n",
    "# Display the table properly in Jupyter Notebook\n",
    "display(df_api_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec68343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 2.x Execution Time: 9.8884 seconds\n"
     ]
    }
   ],
   "source": [
    "# 5. Dataset API Tests (Spark 2.x)\n",
    "# ------------------ Spark 2.x Dataset API Test (100M Rows) ------------------\n",
    "import time\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Dataset API Test - Spark 2.x\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Schema for Dataset API\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Generate Large Dataset Using Row Objects (Simulating Dataset API)\n",
    "NUM_RECORDS = 10**8  # 100 million rows\n",
    "data = [Row(name=f\"Person_{i}\", age=i % 100) for i in range(10**6)]  # Generate 1M at a time\n",
    "\n",
    "# Convert to Dataset (Using DataFrame in PySpark)\n",
    "df_spark2 = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Apply Dataset-style Filtering Operation\n",
    "start_time = time.time()\n",
    "df_spark2_filtered = df_spark2.filter(col(\"age\") > 30).count()\n",
    "end_time = time.time()\n",
    "\n",
    "# Print Execution Time\n",
    "spark2_time = round(end_time - start_time, 4)\n",
    "print(f\"Spark 2.x Execution Time: {spark2_time} seconds\")\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7d483",
   "metadata": {},
   "source": [
    "## Specific Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e06f7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 2.4.8\n"
     ]
    }
   ],
   "source": [
    "# Ensure findspark is initialized (only needed if running outside a Spark environment)\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import Spark modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any existing Spark session (Avoid conflicts)\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass  # Ignore errors if Spark was not running\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame Transformations Test\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984eef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 2.4.8\n",
      "Test DataFrame created successfully!\n"
     ]
    }
   ],
   "source": [
    "# 1. DataFrame Transformations (Spark 2.x)\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Initialize Spark Session\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop existing session & start a new one\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame Transformations Test - Spark 2.x\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Measure execution function\n",
    "def measure_execution(operation_name, func):\n",
    "    start_time = time.time()\n",
    "    result = func()\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    print(f\"Operation: {operation_name}\")\n",
    "    print(f\"  Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    return {\n",
    "        \"operation\": operation_name,\n",
    "        \"execution_time\": execution_time\n",
    "    }\n",
    "\n",
    "# Create test data function\n",
    "def create_test_data(spark, size=100000):\n",
    "    data = [(i, f\"name_{i}\", i % 100, i % 5) for i in range(size)]\n",
    "    df = spark.createDataFrame(data, [\"id\", \"name\", \"value\", \"category\"])\n",
    "    return df\n",
    "\n",
    "# Run test\n",
    "test_df = create_test_data(spark, size=1000000)\n",
    "print(\"Test DataFrame created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7834fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation: Column operations\n",
      "  Execution time: 0.7132 seconds\n",
      "Operation: Complex aggregations\n",
      "  Execution time: 1.7049 seconds\n",
      "Operation: Window functions\n",
      "  Execution time: 1.9442 seconds\n",
      "Operation: Join operations\n",
      "  Execution time: 9.7936 seconds\n"
     ]
    }
   ],
   "source": [
    "# 2. Specific DataFrame Transformation Tests\n",
    "# Test: Column operations\n",
    "results.append(measure_execution(\n",
    "    \"Column operations\", \n",
    "    lambda: test_df.withColumn(\"value_squared\", F.col(\"value\") * F.col(\"value\"))\n",
    "             .withColumn(\"category_str\", F.concat(F.lit(\"Category: \"), F.col(\"category\").cast(\"string\")))\n",
    "             .select(\"id\", \"name\", \"value_squared\", \"category_str\")\n",
    "             .cache()\n",
    "             .count()\n",
    "))\n",
    "\n",
    "# Test: Complex aggregations\n",
    "results.append(measure_execution(\n",
    "    \"Complex aggregations\", \n",
    "    lambda: test_df.groupBy(\"category\")\n",
    "             .agg(\n",
    "                 F.count(\"id\").alias(\"count\"),\n",
    "                 F.sum(\"value\").alias(\"total_value\"),\n",
    "                 F.avg(\"value\").alias(\"avg_value\"),\n",
    "                 F.min(\"value\").alias(\"min_value\"),\n",
    "                 F.max(\"value\").alias(\"max_value\"),\n",
    "                 F.expr(\"percentile(value, 0.5)\").alias(\"median_value\")\n",
    "             )\n",
    "             .orderBy(\"category\")\n",
    "             .collect()\n",
    "))\n",
    "\n",
    "# Test: Window functions\n",
    "results.append(measure_execution(\n",
    "    \"Window functions\", \n",
    "    lambda: test_df.withColumn(\n",
    "                \"rank\", F.rank().over(Window.partitionBy(\"category\").orderBy(F.desc(\"value\")))\n",
    "             )\n",
    "             .withColumn(\n",
    "                \"running_total\", F.sum(\"value\").over(Window.partitionBy(\"category\").orderBy(\"id\").rowsBetween(Window.unboundedPreceding, 0))\n",
    "             )\n",
    "             .where(F.col(\"rank\") <= 10)\n",
    "             .orderBy(\"category\", \"rank\")\n",
    "             .cache()\n",
    "             .count()\n",
    "))\n",
    "\n",
    "# Test: Join operations\n",
    "results.append(measure_execution(\n",
    "    \"Join operations\", \n",
    "    lambda: spark.createDataFrame(\n",
    "            [(i % 5, f\"cat_{i % 5}\") for i in range(5)],\n",
    "            [\"category\", \"category_name\"]\n",
    "        ).join(\n",
    "            test_df,\n",
    "            on=\"category\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        .groupBy(\"category_name\")\n",
    "        .agg(F.count(\"*\").alias(\"count\"), F.avg(\"value\").alias(\"avg_value\"))\n",
    "        .collect()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7301c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Standard UDF processing: 18.0084 sec\n",
      "ðŸ“ Standard UDF Code Complexity: 19 lines\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "import time\n",
    "\n",
    "# âœ… Reduce dataset size\n",
    "NUM_RECORDS = 50000  # Reduced to 50,000 rows\n",
    "data = [(i, f\"Person_{i}\", i % 100, i % 5) for i in range(NUM_RECORDS)]\n",
    "test_df = spark.createDataFrame(data, [\"id\", \"name\", \"value\", \"category\"])\n",
    "\n",
    "# âœ… Optimize Spark Config\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")  # Reduce shuffle partitions\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")  # Disable Arrow to prevent conflicts\n",
    "\n",
    "# âœ… Define Standard UDFs\n",
    "standard_double_udf = udf(lambda x: x * 2 if x is not None else 0, IntegerType())\n",
    "standard_categorize_udf = udf(lambda x: \"high\" if x > 50 else \"low\", StringType())\n",
    "\n",
    "# âœ… Apply UDFs efficiently\n",
    "test_df = test_df.withColumn(\"doubled\", standard_double_udf(F.col(\"value\"))) \\\n",
    "                 .withColumn(\"category_label\", standard_categorize_udf(F.col(\"value\")))\n",
    "\n",
    "# âœ… Persist (instead of cache) to optimize memory\n",
    "test_df.persist()\n",
    "\n",
    "# âœ… Perform aggregation separately (fetch only required results)\n",
    "def safe_measure_execution(operation_name, func):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = func()\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"âœ… {operation_name}: {execution_time:.4f} sec\")\n",
    "        return execution_time\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {operation_name} Failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "results.append(safe_measure_execution(\n",
    "    \"Standard UDF processing\",\n",
    "    lambda: test_df.groupBy(\"category_label\")\n",
    "             .agg(F.avg(\"doubled\").alias(\"avg_doubled\"))\n",
    "             .collect()  # No `.limit()` before collect\n",
    "))\n",
    "\n",
    "# âœ… Code Complexity Measurement\n",
    "standard_udf_code = \"\"\"\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "# Define Standard UDFs\n",
    "standard_double_udf = udf(lambda x: x * 2 if x is not None else 0, IntegerType())\n",
    "standard_categorize_udf = udf(lambda x: \"high\" if x > 50 else \"low\", StringType())\n",
    "\n",
    "# Apply UDFs efficiently\n",
    "test_df = test_df.withColumn(\"doubled\", standard_double_udf(F.col(\"value\"))) \\\n",
    "                 .withColumn(\"category_label\", standard_categorize_udf(F.col(\"value\")))\n",
    "\n",
    "test_df.persist()\n",
    "\n",
    "# Perform aggregation separately\n",
    "results.append(measure_execution(\n",
    "    \"Standard UDF processing\",\n",
    "    lambda: test_df.groupBy(\"category_label\")\n",
    "             .agg(F.avg(\"doubled\").alias(\"avg_doubled\"))\n",
    "             .collect()\n",
    "))\n",
    "\"\"\"\n",
    "\n",
    "# âœ… Measure line count dynamically\n",
    "line_count = len(standard_udf_code.strip().split('\\n'))\n",
    "print(f\"ðŸ“ Standard UDF Code Complexity: {line_count} lines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3cd2aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: DataFrame filtering and aggregation\n",
      "  Execution time: 0.0532 seconds\n",
      "  Code lines: 4\n",
      "Test: Window functions\n",
      "  Execution time: 0.0396 seconds\n",
      "  Code lines: 2\n",
      "Test: Standard UDF processing\n",
      "  Execution time: 0.0581 seconds\n",
      "  Code lines: 6\n",
      "\n",
      "Spark 2.x Metrics Summary:\n",
      "                             test_name spark_version  execution_time  \\\n",
      "0  DataFrame filtering and aggregation           2.x        0.053166   \n",
      "1                     Window functions           2.x        0.039643   \n",
      "2              Standard UDF processing           2.x        0.058070   \n",
      "\n",
      "   code_lines  \n",
      "0           4  \n",
      "1           2  \n",
      "2           6  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Track metrics without sparkmeasure\n",
    "def track_metrics(test_name, operation_func, code_snippet):\n",
    "    # Time measurement\n",
    "    start_time = time.time()\n",
    "    result = operation_func()\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    # Count lines in code snippet\n",
    "    line_count = len(code_snippet.strip().split('\\n'))\n",
    "    \n",
    "    print(f\"Test: {test_name}\")\n",
    "    print(f\"  Execution time: {execution_time:.4f} seconds\")\n",
    "    print(f\"  Code lines: {line_count}\")\n",
    "    \n",
    "    return {\n",
    "        \"test_name\": test_name,\n",
    "        \"spark_version\": \"2.x\",\n",
    "        \"execution_time\": execution_time,\n",
    "        \"code_lines\": line_count\n",
    "    }\n",
    "\n",
    "# Create sample data\n",
    "def create_test_data(size=50000):\n",
    "    data = [(i, f\"name_{i}\", i % 100, i % 5) for i in range(size)]\n",
    "    return spark.createDataFrame(data, [\"id\", \"name\", \"value\", \"category\"])\n",
    "\n",
    "# Create test dataset\n",
    "test_df = create_test_data()\n",
    "metrics = []\n",
    "\n",
    "# Test 1: DataFrame filtering and aggregation\n",
    "test_1_code = \"\"\"\n",
    "filtered_df = test_df.filter(F.col(\"value\") > 50)\n",
    "result = filtered_df.groupBy(\"category\") \\\n",
    "                   .agg(F.count(\"*\").alias(\"count\"), \n",
    "                        F.avg(\"value\").alias(\"avg_value\"),\n",
    "                        F.sum(\"value\").alias(\"sum_value\")) \\\n",
    "                   .orderBy(\"category\")\n",
    "\"\"\"\n",
    "\n",
    "metrics.append(track_metrics(\n",
    "    \"DataFrame filtering and aggregation\",\n",
    "    lambda: eval(compile(test_1_code, \"<string>\", \"exec\"), \n",
    "                {\"test_df\": test_df, \"F\": F}),\n",
    "    test_1_code\n",
    "))\n",
    "\n",
    "# Test 2: Window functions\n",
    "test_2_code = \"\"\"\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(F.desc(\"value\"))\n",
    "result = test_df.withColumn(\"rank\", F.rank().over(window_spec)) \\\n",
    "               .filter(F.col(\"rank\") <= 10) \\\n",
    "               .orderBy(\"category\", \"rank\")\n",
    "\"\"\"\n",
    "\n",
    "metrics.append(track_metrics(\n",
    "    \"Window functions\",\n",
    "    lambda: eval(compile(test_2_code, \"<string>\", \"exec\"), \n",
    "                {\"test_df\": test_df, \"F\": F, \"Window\": Window}),\n",
    "    test_2_code\n",
    "))\n",
    "\n",
    "# Test 3: Standard UDF in Spark 2.x\n",
    "test_3_code = \"\"\"\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "double_value = udf(lambda x: x * 2, IntegerType())\n",
    "categorize = udf(lambda x: \"high\" if x > 50 else \"low\", StringType())\n",
    "\n",
    "result = test_df.withColumn(\"doubled\", double_value(F.col(\"value\"))) \\\n",
    "               .withColumn(\"category_text\", categorize(F.col(\"value\"))) \\\n",
    "               .groupBy(\"category_text\") \\\n",
    "               .agg(F.avg(\"doubled\").alias(\"avg_doubled\"))\n",
    "\"\"\"\n",
    "\n",
    "metrics.append(track_metrics(\n",
    "    \"Standard UDF processing\",\n",
    "    lambda: eval(compile(test_3_code, \"<string>\", \"exec\"), \n",
    "                {\"test_df\": test_df, \"F\": F, \"IntegerType\": IntegerType, \n",
    "                 \"StringType\": StringType}),\n",
    "    test_3_code\n",
    "))\n",
    "\n",
    "# Save results\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(\"\\nSpark 2.x Metrics Summary:\")\n",
    "print(metrics_df)\n",
    "metrics_df.to_csv(\"spark2_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083721e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
